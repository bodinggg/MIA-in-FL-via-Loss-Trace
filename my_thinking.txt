目前想法是，联邦学习中客户端只能获得模型计算出来的损失值，如何通过本地执行的训练过程去侵犯全局模型的隐私。

考虑正常情况下，经过训练的数据集会更容易收敛，未经过训练的数据集训练过程中会容易遇到阻碍，因此在损失轨迹中，两者应该存在直观的差异性。

但是目前实验看，成员与非成员数据集如果分开训练，测试攻击的各项指标都接近满分，如果成员与非成员数据集混合在一起训练的话就根本无法区分，全程在猜。
损失轨迹经过可视化也发现，如果两者混合一起训练执行攻击，绘制的折线很难有区分度，但是成员/非成员的均值是存在差异的。

3_clients_withoutdl.py是攻击者的成员非成员与目标数据的成员与非成员都是单独划分出来执行攻击的。
3_attack_clients.py 是攻击者的成员与非成员分开训练攻击模型，目标数据的成员与非成员是统一到一个shuffle=True的dataloader中执行的。
